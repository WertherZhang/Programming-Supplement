{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TNI5xtZoiUo"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dbamman/anlp25/blob/main/1.words/HW1_Tokenization.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0S5oZORxoiUq"
      },
      "source": [
        "# Homework 1: Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bv5qAYY2vxLH"
      },
      "source": [
        "In this homework, you'll compare the tokenizations outputs from different classes of tokenizers. This homework is also an opportunity for you to check in on your Python proficiency; for all of the operations below (downloading a file, reading it in, counting objects), you should either be comfortable implementing them already or know how to find out how to do so yourself (if you find yourself struggling with them, we encourage you to take this class at a later date, with a bit more Python experience under your belt).\n",
        "\n",
        "We've added some space for you to write the code for each section, but feel free to create more code cells if you'd like."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uzG_NzfvK60"
      },
      "source": [
        "## Part 1\n",
        "\n",
        "Tokenize the following document with each of these models. Feel free to use the documentation linked (and AI Assistance) to do so for this low-level operation (but again remember that you have to be able to explain what your code is doing).  For each of the tokenizers above, we want to see a list of tokens for this document (not numeric token IDs, but legible words) -- e.g., \\[\"London\", \".\", ...\\]\n",
        "\n",
        "* NLTK `word_tokenize` (https://www.nltk.org/book/ch03.html)\n",
        "* Spacy `tokenize` (https://spacy.io/usage/spacy-101#annotations-token)\n",
        "* Tiktoken BPE tokenization (https://github.com/openai/tiktoken) -- cl100k_base (GPT-3.5, GPT-4).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9ywLVgL3v5sy"
      },
      "outputs": [],
      "source": [
        "document = \"London. Michaelmas term lately over, and the Lord Chancellor sitting in Lincoln’s Inn Hall. Implacable November weather. As much mud in the streets as if the waters had but newly retired from the face of the earth, and it would not be wonderful to meet a Megalosaurus, forty feet long or so, waddling like an elephantine lizard up Holborn Hill. Smoke lowering down from chimney-pots, making a soft black drizzle, with flakes of soot in it as big as full-grown snowflakes—gone into mourning, one might imagine, for the death of the sun. Dogs, undistinguishable in mire. Horses, scarcely better; splashed to their very blinkers. Foot passengers, jostling one another’s umbrellas in a general infection of ill temper, and losing their foot-hold at street-corners, where tens of thousands of other foot passengers have been slipping and sliding since the day broke (if this day ever broke), adding new deposits to the crust upon crust of mud, sticking at those points tenaciously to the pavement, and accumulating at compound interest.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hmm9ksPfoiUs",
        "outputId": "f4f1f70b-1cef-4d7a-ec9a-4ff7e5090d95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Your code here:\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "import spacy\n",
        "import tiktoken\n",
        "import os, io, requests, json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk_tokens = word_tokenize(document)"
      ],
      "metadata": {
        "id": "8YxUMrL2o7_O"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"ner\", \"lemmatizer\", \"attribute_ruler\"])\n",
        "spacy_doc = nlp(document)\n",
        "spacy_tokens = [t.text for t in spacy_doc]"
      ],
      "metadata": {
        "id": "iuX9dsyUpYWn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "token_ids = enc.encode(document)\n",
        "\n",
        "token_pieces = [b\"\".join(enc.decode_tokens_bytes([tid])).decode(\"utf-8\", errors=\"replace\")\n",
        "                for tid in token_ids]\n",
        "\n",
        "print(token_pieces[:50])"
      ],
      "metadata": {
        "id": "nS8eJQ0BpbCN",
        "outputId": "a1fd2254-bfe9-4e7c-a9cc-2f9ddc113d9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['London', '.', ' Michael', 'mas', ' term', ' lately', ' over', ',', ' and', ' the', ' Lord', ' Chancellor', ' sitting', ' in', ' Lincoln', '’s', ' Inn', ' Hall', '.', ' Impl', 'ac', 'able', ' November', ' weather', '.', ' As', ' much', ' mud', ' in', ' the', ' streets', ' as', ' if', ' the', ' waters', ' had', ' but', ' newly', ' retired', ' from', ' the', ' face', ' of', ' the', ' earth', ',', ' and', ' it', ' would', ' not']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def show(name, tokens):\n",
        "    print(f\"\\n{name} ({len(tokens)} tokens):\")\n",
        "    print(json.dumps(tokens, ensure_ascii=False, indent=2))\n",
        "\n",
        "show(\"NLTK word_tokenize\", nltk_tokens)\n",
        "show(\"spaCy tokenize\", spacy_tokens)\n",
        "show(\"tiktoken cl100k_base (string pieces)\", token_pieces)"
      ],
      "metadata": {
        "id": "kiEuFlWhpwEJ",
        "outputId": "dba4842a-e997-48d6-8fb6-6fe1b17a6516",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "NLTK word_tokenize (199 tokens):\n",
            "[\n",
            "  \"London\",\n",
            "  \".\",\n",
            "  \"Michaelmas\",\n",
            "  \"term\",\n",
            "  \"lately\",\n",
            "  \"over\",\n",
            "  \",\",\n",
            "  \"and\",\n",
            "  \"the\",\n",
            "  \"Lord\",\n",
            "  \"Chancellor\",\n",
            "  \"sitting\",\n",
            "  \"in\",\n",
            "  \"Lincoln\",\n",
            "  \"’\",\n",
            "  \"s\",\n",
            "  \"Inn\",\n",
            "  \"Hall\",\n",
            "  \".\",\n",
            "  \"Implacable\",\n",
            "  \"November\",\n",
            "  \"weather\",\n",
            "  \".\",\n",
            "  \"As\",\n",
            "  \"much\",\n",
            "  \"mud\",\n",
            "  \"in\",\n",
            "  \"the\",\n",
            "  \"streets\",\n",
            "  \"as\",\n",
            "  \"if\",\n",
            "  \"the\",\n",
            "  \"waters\",\n",
            "  \"had\",\n",
            "  \"but\",\n",
            "  \"newly\",\n",
            "  \"retired\",\n",
            "  \"from\",\n",
            "  \"the\",\n",
            "  \"face\",\n",
            "  \"of\",\n",
            "  \"the\",\n",
            "  \"earth\",\n",
            "  \",\",\n",
            "  \"and\",\n",
            "  \"it\",\n",
            "  \"would\",\n",
            "  \"not\",\n",
            "  \"be\",\n",
            "  \"wonderful\",\n",
            "  \"to\",\n",
            "  \"meet\",\n",
            "  \"a\",\n",
            "  \"Megalosaurus\",\n",
            "  \",\",\n",
            "  \"forty\",\n",
            "  \"feet\",\n",
            "  \"long\",\n",
            "  \"or\",\n",
            "  \"so\",\n",
            "  \",\",\n",
            "  \"waddling\",\n",
            "  \"like\",\n",
            "  \"an\",\n",
            "  \"elephantine\",\n",
            "  \"lizard\",\n",
            "  \"up\",\n",
            "  \"Holborn\",\n",
            "  \"Hill\",\n",
            "  \".\",\n",
            "  \"Smoke\",\n",
            "  \"lowering\",\n",
            "  \"down\",\n",
            "  \"from\",\n",
            "  \"chimney-pots\",\n",
            "  \",\",\n",
            "  \"making\",\n",
            "  \"a\",\n",
            "  \"soft\",\n",
            "  \"black\",\n",
            "  \"drizzle\",\n",
            "  \",\",\n",
            "  \"with\",\n",
            "  \"flakes\",\n",
            "  \"of\",\n",
            "  \"soot\",\n",
            "  \"in\",\n",
            "  \"it\",\n",
            "  \"as\",\n",
            "  \"big\",\n",
            "  \"as\",\n",
            "  \"full-grown\",\n",
            "  \"snowflakes—gone\",\n",
            "  \"into\",\n",
            "  \"mourning\",\n",
            "  \",\",\n",
            "  \"one\",\n",
            "  \"might\",\n",
            "  \"imagine\",\n",
            "  \",\",\n",
            "  \"for\",\n",
            "  \"the\",\n",
            "  \"death\",\n",
            "  \"of\",\n",
            "  \"the\",\n",
            "  \"sun\",\n",
            "  \".\",\n",
            "  \"Dogs\",\n",
            "  \",\",\n",
            "  \"undistinguishable\",\n",
            "  \"in\",\n",
            "  \"mire\",\n",
            "  \".\",\n",
            "  \"Horses\",\n",
            "  \",\",\n",
            "  \"scarcely\",\n",
            "  \"better\",\n",
            "  \";\",\n",
            "  \"splashed\",\n",
            "  \"to\",\n",
            "  \"their\",\n",
            "  \"very\",\n",
            "  \"blinkers\",\n",
            "  \".\",\n",
            "  \"Foot\",\n",
            "  \"passengers\",\n",
            "  \",\",\n",
            "  \"jostling\",\n",
            "  \"one\",\n",
            "  \"another\",\n",
            "  \"’\",\n",
            "  \"s\",\n",
            "  \"umbrellas\",\n",
            "  \"in\",\n",
            "  \"a\",\n",
            "  \"general\",\n",
            "  \"infection\",\n",
            "  \"of\",\n",
            "  \"ill\",\n",
            "  \"temper\",\n",
            "  \",\",\n",
            "  \"and\",\n",
            "  \"losing\",\n",
            "  \"their\",\n",
            "  \"foot-hold\",\n",
            "  \"at\",\n",
            "  \"street-corners\",\n",
            "  \",\",\n",
            "  \"where\",\n",
            "  \"tens\",\n",
            "  \"of\",\n",
            "  \"thousands\",\n",
            "  \"of\",\n",
            "  \"other\",\n",
            "  \"foot\",\n",
            "  \"passengers\",\n",
            "  \"have\",\n",
            "  \"been\",\n",
            "  \"slipping\",\n",
            "  \"and\",\n",
            "  \"sliding\",\n",
            "  \"since\",\n",
            "  \"the\",\n",
            "  \"day\",\n",
            "  \"broke\",\n",
            "  \"(\",\n",
            "  \"if\",\n",
            "  \"this\",\n",
            "  \"day\",\n",
            "  \"ever\",\n",
            "  \"broke\",\n",
            "  \")\",\n",
            "  \",\",\n",
            "  \"adding\",\n",
            "  \"new\",\n",
            "  \"deposits\",\n",
            "  \"to\",\n",
            "  \"the\",\n",
            "  \"crust\",\n",
            "  \"upon\",\n",
            "  \"crust\",\n",
            "  \"of\",\n",
            "  \"mud\",\n",
            "  \",\",\n",
            "  \"sticking\",\n",
            "  \"at\",\n",
            "  \"those\",\n",
            "  \"points\",\n",
            "  \"tenaciously\",\n",
            "  \"to\",\n",
            "  \"the\",\n",
            "  \"pavement\",\n",
            "  \",\",\n",
            "  \"and\",\n",
            "  \"accumulating\",\n",
            "  \"at\",\n",
            "  \"compound\",\n",
            "  \"interest\",\n",
            "  \".\"\n",
            "]\n",
            "\n",
            "spaCy tokenize (207 tokens):\n",
            "[\n",
            "  \"London\",\n",
            "  \".\",\n",
            "  \"Michaelmas\",\n",
            "  \"term\",\n",
            "  \"lately\",\n",
            "  \"over\",\n",
            "  \",\",\n",
            "  \"and\",\n",
            "  \"the\",\n",
            "  \"Lord\",\n",
            "  \"Chancellor\",\n",
            "  \"sitting\",\n",
            "  \"in\",\n",
            "  \"Lincoln\",\n",
            "  \"’s\",\n",
            "  \"Inn\",\n",
            "  \"Hall\",\n",
            "  \".\",\n",
            "  \"Implacable\",\n",
            "  \"November\",\n",
            "  \"weather\",\n",
            "  \".\",\n",
            "  \"As\",\n",
            "  \"much\",\n",
            "  \"mud\",\n",
            "  \"in\",\n",
            "  \"the\",\n",
            "  \"streets\",\n",
            "  \"as\",\n",
            "  \"if\",\n",
            "  \"the\",\n",
            "  \"waters\",\n",
            "  \"had\",\n",
            "  \"but\",\n",
            "  \"newly\",\n",
            "  \"retired\",\n",
            "  \"from\",\n",
            "  \"the\",\n",
            "  \"face\",\n",
            "  \"of\",\n",
            "  \"the\",\n",
            "  \"earth\",\n",
            "  \",\",\n",
            "  \"and\",\n",
            "  \"it\",\n",
            "  \"would\",\n",
            "  \"not\",\n",
            "  \"be\",\n",
            "  \"wonderful\",\n",
            "  \"to\",\n",
            "  \"meet\",\n",
            "  \"a\",\n",
            "  \"Megalosaurus\",\n",
            "  \",\",\n",
            "  \"forty\",\n",
            "  \"feet\",\n",
            "  \"long\",\n",
            "  \"or\",\n",
            "  \"so\",\n",
            "  \",\",\n",
            "  \"waddling\",\n",
            "  \"like\",\n",
            "  \"an\",\n",
            "  \"elephantine\",\n",
            "  \"lizard\",\n",
            "  \"up\",\n",
            "  \"Holborn\",\n",
            "  \"Hill\",\n",
            "  \".\",\n",
            "  \"Smoke\",\n",
            "  \"lowering\",\n",
            "  \"down\",\n",
            "  \"from\",\n",
            "  \"chimney\",\n",
            "  \"-\",\n",
            "  \"pots\",\n",
            "  \",\",\n",
            "  \"making\",\n",
            "  \"a\",\n",
            "  \"soft\",\n",
            "  \"black\",\n",
            "  \"drizzle\",\n",
            "  \",\",\n",
            "  \"with\",\n",
            "  \"flakes\",\n",
            "  \"of\",\n",
            "  \"soot\",\n",
            "  \"in\",\n",
            "  \"it\",\n",
            "  \"as\",\n",
            "  \"big\",\n",
            "  \"as\",\n",
            "  \"full\",\n",
            "  \"-\",\n",
            "  \"grown\",\n",
            "  \"snowflakes\",\n",
            "  \"—\",\n",
            "  \"gone\",\n",
            "  \"into\",\n",
            "  \"mourning\",\n",
            "  \",\",\n",
            "  \"one\",\n",
            "  \"might\",\n",
            "  \"imagine\",\n",
            "  \",\",\n",
            "  \"for\",\n",
            "  \"the\",\n",
            "  \"death\",\n",
            "  \"of\",\n",
            "  \"the\",\n",
            "  \"sun\",\n",
            "  \".\",\n",
            "  \"Dogs\",\n",
            "  \",\",\n",
            "  \"undistinguishable\",\n",
            "  \"in\",\n",
            "  \"mire\",\n",
            "  \".\",\n",
            "  \"Horses\",\n",
            "  \",\",\n",
            "  \"scarcely\",\n",
            "  \"better\",\n",
            "  \";\",\n",
            "  \"splashed\",\n",
            "  \"to\",\n",
            "  \"their\",\n",
            "  \"very\",\n",
            "  \"blinkers\",\n",
            "  \".\",\n",
            "  \"Foot\",\n",
            "  \"passengers\",\n",
            "  \",\",\n",
            "  \"jostling\",\n",
            "  \"one\",\n",
            "  \"another\",\n",
            "  \"’s\",\n",
            "  \"umbrellas\",\n",
            "  \"in\",\n",
            "  \"a\",\n",
            "  \"general\",\n",
            "  \"infection\",\n",
            "  \"of\",\n",
            "  \"ill\",\n",
            "  \"temper\",\n",
            "  \",\",\n",
            "  \"and\",\n",
            "  \"losing\",\n",
            "  \"their\",\n",
            "  \"foot\",\n",
            "  \"-\",\n",
            "  \"hold\",\n",
            "  \"at\",\n",
            "  \"street\",\n",
            "  \"-\",\n",
            "  \"corners\",\n",
            "  \",\",\n",
            "  \"where\",\n",
            "  \"tens\",\n",
            "  \"of\",\n",
            "  \"thousands\",\n",
            "  \"of\",\n",
            "  \"other\",\n",
            "  \"foot\",\n",
            "  \"passengers\",\n",
            "  \"have\",\n",
            "  \"been\",\n",
            "  \"slipping\",\n",
            "  \"and\",\n",
            "  \"sliding\",\n",
            "  \"since\",\n",
            "  \"the\",\n",
            "  \"day\",\n",
            "  \"broke\",\n",
            "  \"(\",\n",
            "  \"if\",\n",
            "  \"this\",\n",
            "  \"day\",\n",
            "  \"ever\",\n",
            "  \"broke\",\n",
            "  \")\",\n",
            "  \",\",\n",
            "  \"adding\",\n",
            "  \"new\",\n",
            "  \"deposits\",\n",
            "  \"to\",\n",
            "  \"the\",\n",
            "  \"crust\",\n",
            "  \"upon\",\n",
            "  \"crust\",\n",
            "  \"of\",\n",
            "  \"mud\",\n",
            "  \",\",\n",
            "  \"sticking\",\n",
            "  \"at\",\n",
            "  \"those\",\n",
            "  \"points\",\n",
            "  \"tenaciously\",\n",
            "  \"to\",\n",
            "  \"the\",\n",
            "  \"pavement\",\n",
            "  \",\",\n",
            "  \"and\",\n",
            "  \"accumulating\",\n",
            "  \"at\",\n",
            "  \"compound\",\n",
            "  \"interest\",\n",
            "  \".\"\n",
            "]\n",
            "\n",
            "tiktoken cl100k_base (string pieces) (231 tokens):\n",
            "[\n",
            "  \"London\",\n",
            "  \".\",\n",
            "  \" Michael\",\n",
            "  \"mas\",\n",
            "  \" term\",\n",
            "  \" lately\",\n",
            "  \" over\",\n",
            "  \",\",\n",
            "  \" and\",\n",
            "  \" the\",\n",
            "  \" Lord\",\n",
            "  \" Chancellor\",\n",
            "  \" sitting\",\n",
            "  \" in\",\n",
            "  \" Lincoln\",\n",
            "  \"’s\",\n",
            "  \" Inn\",\n",
            "  \" Hall\",\n",
            "  \".\",\n",
            "  \" Impl\",\n",
            "  \"ac\",\n",
            "  \"able\",\n",
            "  \" November\",\n",
            "  \" weather\",\n",
            "  \".\",\n",
            "  \" As\",\n",
            "  \" much\",\n",
            "  \" mud\",\n",
            "  \" in\",\n",
            "  \" the\",\n",
            "  \" streets\",\n",
            "  \" as\",\n",
            "  \" if\",\n",
            "  \" the\",\n",
            "  \" waters\",\n",
            "  \" had\",\n",
            "  \" but\",\n",
            "  \" newly\",\n",
            "  \" retired\",\n",
            "  \" from\",\n",
            "  \" the\",\n",
            "  \" face\",\n",
            "  \" of\",\n",
            "  \" the\",\n",
            "  \" earth\",\n",
            "  \",\",\n",
            "  \" and\",\n",
            "  \" it\",\n",
            "  \" would\",\n",
            "  \" not\",\n",
            "  \" be\",\n",
            "  \" wonderful\",\n",
            "  \" to\",\n",
            "  \" meet\",\n",
            "  \" a\",\n",
            "  \" Meg\",\n",
            "  \"al\",\n",
            "  \"os\",\n",
            "  \"aurus\",\n",
            "  \",\",\n",
            "  \" forty\",\n",
            "  \" feet\",\n",
            "  \" long\",\n",
            "  \" or\",\n",
            "  \" so\",\n",
            "  \",\",\n",
            "  \" w\",\n",
            "  \"add\",\n",
            "  \"ling\",\n",
            "  \" like\",\n",
            "  \" an\",\n",
            "  \" elephant\",\n",
            "  \"ine\",\n",
            "  \" lizard\",\n",
            "  \" up\",\n",
            "  \" Hol\",\n",
            "  \"born\",\n",
            "  \" Hill\",\n",
            "  \".\",\n",
            "  \" Smoke\",\n",
            "  \" lowering\",\n",
            "  \" down\",\n",
            "  \" from\",\n",
            "  \" chimney\",\n",
            "  \"-p\",\n",
            "  \"ots\",\n",
            "  \",\",\n",
            "  \" making\",\n",
            "  \" a\",\n",
            "  \" soft\",\n",
            "  \" black\",\n",
            "  \" dr\",\n",
            "  \"izzle\",\n",
            "  \",\",\n",
            "  \" with\",\n",
            "  \" flakes\",\n",
            "  \" of\",\n",
            "  \" so\",\n",
            "  \"ot\",\n",
            "  \" in\",\n",
            "  \" it\",\n",
            "  \" as\",\n",
            "  \" big\",\n",
            "  \" as\",\n",
            "  \" full\",\n",
            "  \"-g\",\n",
            "  \"rown\",\n",
            "  \" snow\",\n",
            "  \"fl\",\n",
            "  \"akes\",\n",
            "  \"—\",\n",
            "  \"gone\",\n",
            "  \" into\",\n",
            "  \" mourning\",\n",
            "  \",\",\n",
            "  \" one\",\n",
            "  \" might\",\n",
            "  \" imagine\",\n",
            "  \",\",\n",
            "  \" for\",\n",
            "  \" the\",\n",
            "  \" death\",\n",
            "  \" of\",\n",
            "  \" the\",\n",
            "  \" sun\",\n",
            "  \".\",\n",
            "  \" Dogs\",\n",
            "  \",\",\n",
            "  \" und\",\n",
            "  \"istinguish\",\n",
            "  \"able\",\n",
            "  \" in\",\n",
            "  \" m\",\n",
            "  \"ire\",\n",
            "  \".\",\n",
            "  \" H\",\n",
            "  \"orses\",\n",
            "  \",\",\n",
            "  \" scarcely\",\n",
            "  \" better\",\n",
            "  \";\",\n",
            "  \" spl\",\n",
            "  \"ashed\",\n",
            "  \" to\",\n",
            "  \" their\",\n",
            "  \" very\",\n",
            "  \" blink\",\n",
            "  \"ers\",\n",
            "  \".\",\n",
            "  \" Foot\",\n",
            "  \" passengers\",\n",
            "  \",\",\n",
            "  \" j\",\n",
            "  \"ost\",\n",
            "  \"ling\",\n",
            "  \" one\",\n",
            "  \" another\",\n",
            "  \"’s\",\n",
            "  \" umb\",\n",
            "  \"rellas\",\n",
            "  \" in\",\n",
            "  \" a\",\n",
            "  \" general\",\n",
            "  \" infection\",\n",
            "  \" of\",\n",
            "  \" ill\",\n",
            "  \" temper\",\n",
            "  \",\",\n",
            "  \" and\",\n",
            "  \" losing\",\n",
            "  \" their\",\n",
            "  \" foot\",\n",
            "  \"-h\",\n",
            "  \"old\",\n",
            "  \" at\",\n",
            "  \" street\",\n",
            "  \"-c\",\n",
            "  \"orners\",\n",
            "  \",\",\n",
            "  \" where\",\n",
            "  \" tens\",\n",
            "  \" of\",\n",
            "  \" thousands\",\n",
            "  \" of\",\n",
            "  \" other\",\n",
            "  \" foot\",\n",
            "  \" passengers\",\n",
            "  \" have\",\n",
            "  \" been\",\n",
            "  \" slipping\",\n",
            "  \" and\",\n",
            "  \" sliding\",\n",
            "  \" since\",\n",
            "  \" the\",\n",
            "  \" day\",\n",
            "  \" broke\",\n",
            "  \" (\",\n",
            "  \"if\",\n",
            "  \" this\",\n",
            "  \" day\",\n",
            "  \" ever\",\n",
            "  \" broke\",\n",
            "  \"),\",\n",
            "  \" adding\",\n",
            "  \" new\",\n",
            "  \" deposits\",\n",
            "  \" to\",\n",
            "  \" the\",\n",
            "  \" crust\",\n",
            "  \" upon\",\n",
            "  \" crust\",\n",
            "  \" of\",\n",
            "  \" mud\",\n",
            "  \",\",\n",
            "  \" sticking\",\n",
            "  \" at\",\n",
            "  \" those\",\n",
            "  \" points\",\n",
            "  \" ten\",\n",
            "  \"ac\",\n",
            "  \"iously\",\n",
            "  \" to\",\n",
            "  \" the\",\n",
            "  \" pavement\",\n",
            "  \",\",\n",
            "  \" and\",\n",
            "  \" accumulating\",\n",
            "  \" at\",\n",
            "  \" compound\",\n",
            "  \" interest\",\n",
            "  \".\"\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbhjqG9m05yS"
      },
      "source": [
        "## Part 2\n",
        "\n",
        "Examine the different tokenizations for the passage above -- i.e., actually read through them and see how they differ. In a paragraph or two, characterize the salient differences in tokenization between a.) NLTK and Spacy and b.) NLTK and BPE.  Reference real examples in the text. At the end of this homework, you want to be able to discuss the practical differences between tokenization methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORh6EcWgoiUt"
      },
      "source": [
        "**Response**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BuULiDfoiUt"
      },
      "source": [
        "Your response here\n",
        "\n",
        "NLTK vs spaCy: differences are in linguistic fidelity. spaCy preserves contractions and hyphenated words more naturally, which is often desirable in linguistic or syntactic tasks. NLTK breaks them more, which may be useful for strict matching but loses information.\n",
        "\n",
        "\n",
        "NLTK vs BPE: differences are in granularity. NLTK assumes words are the smallest unit, while BPE works on subword units to balance vocabulary size with coverage. This makes BPE far more suitable for large-scale language models (like GPT), but it produces tokens that don’t always align with human intuitions about “words.”"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwhNqD61uZi4"
      },
      "source": [
        "## Part 3\n",
        "\n",
        "Download the full text of *Pride and Prejudice* (https://raw.githubusercontent.com/dbamman/anlp25/main/data/1342_pride_and_prejudice.txt) and tokenize it using each of the methods above. How many word types (in the formal sense we discussed in class) does each tokenization method have for that complete file?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "U03MlHL3oiUt"
      },
      "outputs": [],
      "source": [
        "# Your code here:\n",
        "try:\n",
        "    nltk.data.find(\"tokenizers/punkt\")\n",
        "except LookupError:\n",
        "    nltk.download(\"punkt\")\n",
        "try:\n",
        "    nltk.data.find(\"tokenizers/punkt_tab\")\n",
        "except LookupError:\n",
        "    nltk.download(\"punkt_tab\")\n",
        "\n",
        "URL = \"https://raw.githubusercontent.com/dbamman/anlp25/main/data/1342_pride_and_prejudice.txt\"\n",
        "LOCAL_FNAME = \"1342_pride_and_prejudice.txt\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_text():\n",
        "    if os.path.exists(LOCAL_FNAME):\n",
        "        with io.open(LOCAL_FNAME, \"r\", encoding=\"utf-8\") as f:\n",
        "            return f.read()\n",
        "    resp = requests.get(URL, timeout=60)\n",
        "    resp.raise_for_status()\n",
        "    text = resp.text\n",
        "    with io.open(LOCAL_FNAME, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(text)\n",
        "    return text\n",
        "\n",
        "text = load_text()"
      ],
      "metadata": {
        "id": "_vF47isaqax4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk_tokens = word_tokenize(text)\n",
        "nltk_types = set(nltk_tokens)"
      ],
      "metadata": {
        "id": "uxbjA9YwqnXn"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\n",
        "    \"en_core_web_sm\",\n",
        "    disable=[\"tagger\",\"parser\",\"ner\",\"lemmatizer\",\"attribute_ruler\"]\n",
        ")\n",
        "spacy_tokens = [t.text for t in nlp.make_doc(text)]\n",
        "spacy_types = set(spacy_tokens)"
      ],
      "metadata": {
        "id": "jkp1u1YcqqEf"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "bpe_ids = enc.encode(text)\n",
        "\n",
        "bpe_tokens = [\n",
        "    enc.decode_single_token_bytes(tid).decode(\"utf-8\", errors=\"replace\")\n",
        "    for tid in bpe_ids\n",
        "]\n",
        "bpe_types = set(bpe_tokens)"
      ],
      "metadata": {
        "id": "ERgOd9jIqsC0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_count(name, tokens, typeset):\n",
        "    print(f\"{name}:\")\n",
        "    print(f\"  tokens: {len(tokens):,}\")\n",
        "    print(f\"  UNIQUE WORD TYPES: {len(typeset):,}\\n\")\n",
        "\n",
        "show_count(\"NLTK word_tokenize\", nltk_tokens, nltk_types)\n",
        "show_count(\"spaCy tokenize\", spacy_tokens, spacy_types)\n",
        "show_count(\"tiktoken cl100k_base (BPE pieces)\", bpe_tokens, bpe_types)"
      ],
      "metadata": {
        "id": "byyjycNcqu1u",
        "outputId": "44883b9e-fe95-4842-cdc5-e942538bfc19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK word_tokenize:\n",
            "  tokens: 142,522\n",
            "  UNIQUE WORD TYPES: 7,475\n",
            "\n",
            "spaCy tokenize:\n",
            "  tokens: 155,437\n",
            "  UNIQUE WORD TYPES: 6,780\n",
            "\n",
            "tiktoken cl100k_base (BPE pieces):\n",
            "  tokens: 161,075\n",
            "  UNIQUE WORD TYPES: 8,364\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vf1sRWp3LOb"
      },
      "source": [
        "## Part 4\n",
        "\n",
        "Which text has the greater type-token ratio, *Pride and Prejudice* (https://raw.githubusercontent.com/dbamman/anlp25/main/data/1342_pride_and_prejudice.txt) or *Emma* (https://raw.githubusercontent.com/dbamman/anlp25/main/data/158_emma.txt)?  Calculate the TTR for both texts using the NLTK tokenizer, but only use the first 1,000 tokens from each text when calculating its TTR."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "yZmsSK5SoiUu",
        "outputId": "b9bde051-8de6-49f3-f7eb-3f12fb14fefe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The TTR for 'Pride and Prejudice' is 0.404\n",
            "The TTR for 'Emma' is 0.425\n",
            "Emma has the higher TTR.\n"
          ]
        }
      ],
      "source": [
        "# Your code here:\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "pp_url = \"https://raw.githubusercontent.com/dbamman/anlp25/main/data/1342_pride_and_prejudice.txt\"\n",
        "emma_url = \"https://raw.githubusercontent.com/dbamman/anlp25/main/data/158_emma.txt\"\n",
        "\n",
        "pp_text = requests.get(pp_url).text\n",
        "emma_text = requests.get(emma_url).text\n",
        "\n",
        "tok = TreebankWordTokenizer()\n",
        "\n",
        "def ttr_first_k(text, k=1000):\n",
        "    toks = tok.tokenize(text)[:k]\n",
        "    return len(set(toks)) / len(toks)\n",
        "\n",
        "pp_ttr = ttr_first_k(pp_text, 1000)\n",
        "emma_ttr = ttr_first_k(emma_text, 1000)\n",
        "\n",
        "answer = \"Emma\" if emma_ttr > pp_ttr else \"Pride and Prejudice\"\n",
        "\n",
        "print(\"The TTR for 'Pride and Prejudice' is\", pp_ttr)\n",
        "print(\"The TTR for 'Emma' is\", emma_ttr)\n",
        "print(f\"{answer} has the higher TTR.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}