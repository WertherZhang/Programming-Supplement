{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpI4HRGQaTwV"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dbamman/anlp25/blob/main/2.compare/Log_odds_ratio_TODO.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGqPjBxoaTwW"
      },
      "source": [
        "# Log odds-ratio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIVpdSX0E9Tw"
      },
      "source": [
        "The log odds ratio with an informative (and uninformative) Dirichlet prior (described in [Monroe et al. 2009, Fighting Words](http://languagelog.ldc.upenn.edu/myl/Monroe.pdf)) is a common method for finding distinctive terms in two datasets (see [Jurafsky et al. 2014](https://firstmonday.org/ojs/index.php/fm/article/view/4944/3863) for an example article that uses it to make an empirical argument). This method for finding distinguishing words combines a number of desirable properties:\n",
        "\n",
        "* it specifies an intuitive metric (the log-odds) for the ratio of two probabilities\n",
        "* it can incorporate prior information in the form of pseudocounts, which can either act as a smoothing factor (in the uninformative case) or incorporate real information about the expected frequency of words overall.\n",
        "* it accounts for variability of a frequency estimate by essentially converting the log-odds to a z-score.\n",
        "\n",
        "In this homework you will implement this ratio for a dataset of your choice to characterize the words that differentiate each one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7sezbdfE9Ty"
      },
      "source": [
        "## Part 1\n",
        "\n",
        "Your first job is to find two datasets with some interesting opposition -- e.g., news articles from CNN vs. FoxNews, books written by Charles Dickens vs. James Joyce, screenplays of dramas vs. comedies.  Be creative -- this should be driven by what interests you and should reflect your own originality. **This dataset cannot come from Kaggle**.  Feel feel to use web scraping (see [here](https://github.com/CU-ITSS/Web-Data-Scraping-S2023) for a great tutorial) or manually copying/pasting text.  Aim for more than 10,000 tokens for each dataset.\n",
        "   \n",
        "Save those datasets in two files: \"class1_dataset.txt\" and \"class2_dataset.txt\"\n",
        "\n",
        "**Describe each of those datasets and their source in 100-200 words.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuWRsJqZaTwX"
      },
      "source": [
        "Type your response here:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install requests beautifulsoup4 tqdm"
      ],
      "metadata": {
        "id": "6ATpirzNbfj1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TOPICS = [\n",
        "    \"Abortion\", \"Gun_control\", \"Climate_change\", \"Feminism\", \"Immigration\",\n",
        "    \"Same-sex_marriage\", \"Evolution\", \"Creationism\", \"Vaccination\",\n",
        "    \"COVID-19_pandemic\", \"Transgender\", \"Affirmative_action\",\n",
        "    \"Capital_punishment\", \"Socialism\", \"Free_market\", \"Universal_health_care\",\n",
        "    \"Net_neutrality\", \"Renewable_energy\", \"Nuclear_power\", \"Gun_rights\",\n",
        "    \"Intelligent_design\", \"Environmentalism\", \"BLM\", \"Woke\", \"Cultural_Marxism\",\n",
        "    \"Critical_race_theory\", \"Gender_identity\", \"School_choice\", \"Tax_cuts\",\n",
        "    \"Minimum_wage\", \"Universal_basic_income\", \"Welfare_state\", \"Secularism\",\n",
        "    \"Christian_right\", \"LGBT_rights\", \"Great_Replacement\", \"Deep_state\",\n",
        "    \"COVID-19_vaccine\", \"Mask_mandate\", \"Globalism\", \"Nationalism\",\n",
        "    \"Multiculturalism\", \"Law_and_order\", \"Police_reform\", \"Media_bias\",\n",
        "    \"Fake_news\", \"Abstinence-only_sex_education\", \"Sex_education\",\n",
        "    \"Stem_cell_research\", \"Gun_violence\"\n",
        "]\n",
        "\n",
        "WIKI_REST = \"https://en.wikipedia.org/api/rest_v1/page/plain/{title}\"\n",
        "CONSERVA_RENDER = \"https://www.conservapedia.com/index.php?title={title}&action=render\"\n",
        "\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"academic-research-notebook/1.0 (non-commercial; contact: student@example.edu)\"\n",
        "}\n",
        "\n",
        "MIN_TOKENS_PER_CORPUS = 12000"
      ],
      "metadata": {
        "id": "I76VXFNmbhQq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re, time, random, requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import quote\n",
        "\n",
        "def sanitize(text: str) -> str:\n",
        "    text = re.sub(r\"\\[\\d+\\]\", \" \", text)\n",
        "    text = re.sub(r\"\\{\\{.*?\\}\\}\", \" \", text, flags=re.S)\n",
        "    text = re.sub(r\"<ref.*?>.*?</ref>\", \" \", text, flags=re.S)\n",
        "    text = re.sub(r\"<.*?>\", \" \", text, flags=re.S)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "def approx_token_count(text: str) -> int:\n",
        "    return len(re.findall(r\"\\w+\", text))\n",
        "\n",
        "def fetch_wikipedia(title: str) -> str:\n",
        "    url = WIKI_REST.format(title=quote(title))\n",
        "    r = requests.get(url, headers=HEADERS, timeout=30)\n",
        "    if r.status_code == 200 and r.text:\n",
        "        return sanitize(r.text)\n",
        "    return \"\"\n",
        "\n",
        "def fetch_conservapedia(title: str) -> str:\n",
        "    url = CONSERVA_RENDER.format(title=quote(title))\n",
        "    r = requests.get(url, headers=HEADERS, timeout=30)\n",
        "    if r.status_code == 200 and r.text:\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "        text = soup.get_text(separator=\" \")\n",
        "        return sanitize(text)\n",
        "    return \"\"\n",
        "\n",
        "def write_with_license(path: str, corpus_text: str, source_label: str):\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\n",
        "            f\"### Dataset: {path}\\n\"\n",
        "            f\"### Source: {source_label}\\n\"\n",
        "            \"### Licensing:\\n\"\n",
        "            \"- Wikipedia: CC BY-SA 4.0 https://creativecommons.org/licenses/by-sa/4.0/\\n\"\n",
        "            \"- Conservapedia: CC BY-SA 3.0 https://creativecommons.org/licenses/by-sa/3.0/\\n\"\n",
        "            \"### Attribution:\\n\"\n",
        "            \"- Each section is prefixed with SOURCE_ARTICLE followed by the original page title.\\n\"\n",
        "            \"=============================================================\\n\"\n",
        "        )\n",
        "        f.write(corpus_text)\n"
      ],
      "metadata": {
        "id": "In0sBOW4blVP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def build_dataset(fetch_fn, name: str, topics, min_tokens=MIN_TOKENS_PER_CORPUS,\n",
        "                  cool_down=(0.7, 1.6), max_errors=8) -> str:\n",
        "    topics = topics[:]  # copy\n",
        "    random.shuffle(topics)\n",
        "    chunks, total, errors = [], 0, 0\n",
        "    pbar = tqdm(total=min_tokens, desc=f\"Building {name}\", unit=\"tok\")\n",
        "\n",
        "    for t in topics:\n",
        "        try:\n",
        "            txt = fetch_fn(t)\n",
        "            if not txt or len(txt) < 500:\n",
        "                continue\n",
        "            header = f\"\\n\\n===== SOURCE_ARTICLE: {t.replace('_',' ')} =====\\n\"\n",
        "            chunks.append(header + txt)\n",
        "            gained = approx_token_count(txt)\n",
        "            total += gained\n",
        "            pbar.update(gained if total <= min_tokens else max(0, min_tokens - (total - gained)))\n",
        "            if total >= min_tokens:\n",
        "                break\n",
        "            time.sleep(random.uniform(*cool_down))\n",
        "        except Exception:\n",
        "            errors += 1\n",
        "            if errors >= max_errors:\n",
        "                print(f\"[WARN] {name} reached error cap; continuing with collected text.\")\n",
        "                break\n",
        "            continue\n",
        "    pbar.close()\n",
        "    return \"\\n\".join(chunks)"
      ],
      "metadata": {
        "id": "JLHS651wbtNS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_text = build_dataset(fetch_wikipedia, \"Wikipedia\", TOPICS)\n",
        "conserva_text = build_dataset(fetch_conservapedia, \"Conservapedia\", TOPICS)\n",
        "\n",
        "write_with_license(\"class1_dataset.txt\", wiki_text, \"Wikipedia (CC BY-SA 4.0)\")\n",
        "write_with_license(\"class2_dataset.txt\", conserva_text, \"Conservapedia (CC BY-SA 3.0)\")\n",
        "\n",
        "print(\"Saved:\\n - class1_dataset.txt (Wikipedia)\\n - class2_dataset.txt (Conservapedia)\")"
      ],
      "metadata": {
        "id": "Mm5DPqNSbwcb",
        "outputId": "2058e142-e702-4c74-a1fb-aea8cf84d174",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building Wikipedia:   0%|          | 0/12000 [00:04<?, ?tok/s]\n",
            "Building Conservapedia: 100%|██████████| 12000/12000 [00:15<00:00, 752.96tok/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved:\n",
            " - class1_dataset.txt (Wikipedia)\n",
            " - class2_dataset.txt (Conservapedia)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"class1_dataset.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    c1 = f.read()\n",
        "with open(\"class2_dataset.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    c2 = f.read()\n",
        "\n",
        "print(\"Approx tokens:\")\n",
        "print(\"class1_dataset.txt:\", approx_token_count(c1))\n",
        "print(\"class2_dataset.txt:\", approx_token_count(c2))"
      ],
      "metadata": {
        "id": "dzXYC2P6b0qQ",
        "outputId": "fc6d14f0-9701-451d-c6c5-c60edb929990",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Approx tokens:\n",
            "class1_dataset.txt: 52\n",
            "class2_dataset.txt: 14595\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO4_1a4EE9Ty"
      },
      "source": [
        "## Part 2\n",
        "\n",
        "Tokenize those texts by filling out the `read_and_tokenize` function below (your choice of tokenizer). The input is a filename and the output should be a list of tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6eYQtYiCE9Tz"
      },
      "outputs": [],
      "source": [
        "def read_and_tokenize(filename: str) -> list[str]:\n",
        "    \"\"\"Read the file and output a list of strings (tokens).\"\"\"\n",
        "    # your code here\n",
        "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "    tokens = re.findall(r\"\\b\\w+\\b\", text.lower())\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "z28xc1Y1E9Tz"
      },
      "outputs": [],
      "source": [
        "# change these file paths to wherever the datasets you created above live.\n",
        "class1_tokens = read_and_tokenize(\"class1_dataset.txt\")\n",
        "class2_tokens = read_and_tokenize(\"class2_dataset.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"class1 tokens:\", len(class1_tokens))\n",
        "print(\"class2 tokens:\", len(class2_tokens))"
      ],
      "metadata": {
        "id": "JYoM6naMcXpe",
        "outputId": "da10b525-cfc7-48a7-f804-8c6de39444ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class1 tokens: 52\n",
            "class2 tokens: 14595\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPHj4k4tE9Tz"
      },
      "source": [
        "## Part 3\n",
        "\n",
        "Now let's find the words that characterize each of those sources (with respect to the other). Implement the log-odds ratio with an uninformative Dirichlet prior. This value, $\\widehat\\zeta_w^{(i-j)}$ for word $w$ reflecting the difference in usage between corpus $i$ and corpus $j$, is given by the following equation:\n",
        "\n",
        "$$\n",
        "\\widehat{\\zeta}_w^{(i-j)}= {\\widehat{d}_w^{(i-j)} \\over \\sqrt{\\sigma^2\\left(\\widehat{d}_w^{(i-j)}\\right)}}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "$$\n",
        "\\widehat{d}_w^{(i-j)} = \\log \\left({y_w^i + \\alpha_w} \\over {n^i + \\alpha_0 - y_w^i - \\alpha_w}) \\right) -  \\log \\left({y_w^j + \\alpha_w} \\over {n^j + \\alpha_0 - y_w^j - \\alpha_w}) \\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\sigma^2\\left(\\widehat{d}_w^{(i-j)}\\right) \\approx {1 \\over {y_w^i + \\alpha_w}} + {1 \\over {y_w^j + \\alpha_w} }\n",
        "$$\n",
        "\n",
        "And:\n",
        "\n",
        "* $y_w^i = $ count of word $w$ in corpus $i$ (likewise for $j$)\n",
        "* $\\alpha_w$ = 0.01\n",
        "* $V$ = size of vocabulary (number of distinct word types)\n",
        "* $\\alpha_0 = V * \\alpha_w$\n",
        "* $n^i = $ number of words in corpus $i$ (likewise for $j$)\n",
        "\n",
        "In this example, the two corpora are your class1 dataset (e.g., $i$ = your class1) and your class2 dataset (e.g., $j$ = class2). Using this metric, print out the 25 words most strongly aligned with class1, and 25 words most strongly aligned with class2.  Again, consult [Monroe et al. 2009, Fighting Words](http://languagelog.ldc.upenn.edu/myl/Monroe.pdf) for more detail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "lHWahiy8E9T0"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "def logodds_with_uninformative_prior(tokens_i: list[str], tokens_j: list[str], display=25):\n",
        "    \"\"\"Print out the log odds results given two lists of tokens.\"\"\"\n",
        "    # your code here\n",
        "    counts_i = Counter(tokens_i)\n",
        "    counts_j = Counter(tokens_j)\n",
        "    vocab = set(counts_i.keys()) | set(counts_j.keys())\n",
        "    V = len(vocab)\n",
        "\n",
        "    alpha_w = 0.01\n",
        "    alpha_0 = V * alpha_w\n",
        "    n_i = len(tokens_i)\n",
        "    n_j = len(tokens_j)\n",
        "\n",
        "    z_scores = {}\n",
        "\n",
        "    for w in vocab:\n",
        "        y_iw = counts_i.get(w, 0)\n",
        "        y_jw = counts_j.get(w, 0)\n",
        "\n",
        "        d_w = (\n",
        "            math.log((y_iw + alpha_w) / (n_i + alpha_0 - y_iw - alpha_w))\n",
        "            - math.log((y_jw + alpha_w) / (n_j + alpha_0 - y_jw - alpha_w))\n",
        "        )\n",
        "\n",
        "        sigma_sq = 1 / (y_iw + alpha_w) + 1 / (y_jw + alpha_w)\n",
        "\n",
        "        # z-score\n",
        "        z_w = d_w / math.sqrt(sigma_sq)\n",
        "        z_scores[w] = z_w\n",
        "\n",
        "    sorted_i = sorted(z_scores.items(), key=lambda x: x[1], reverse=True)[:display]\n",
        "    sorted_j = sorted(z_scores.items(), key=lambda x: x[1])[:display]\n",
        "\n",
        "    print(f\"Top {display} words aligned with class1:\")\n",
        "    for w, score in sorted_i:\n",
        "        print(f\"{w}\\t{score:.4f}\")\n",
        "\n",
        "    print(\"\\nTop {display} words aligned with class2:\")\n",
        "    for w, score in sorted_j:\n",
        "        print(f\"{w}\\t{score:.4f}\")\n",
        "\n",
        "    return sorted_i, sorted_j"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Ddu4uK9pE9T0",
        "outputId": "185301bd-d964-4048-d106-480f22c3dce8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 25 words aligned with class1:\n",
            "sa\t8.1048\n",
            "0\t7.8606\n",
            "cc\t6.2545\n",
            "4\t5.8437\n",
            "by\t5.6242\n",
            "creativecommons\t5.1000\n",
            "wikipedia\t5.1000\n",
            "licenses\t5.1000\n",
            "org\t4.7965\n",
            "3\t4.1695\n",
            "attribution\t3.6074\n",
            "title\t3.6074\n",
            "prefixed\t3.6074\n",
            "section\t3.6074\n",
            "dataset\t3.6074\n",
            "licensing\t3.6074\n",
            "txt\t3.6074\n",
            "original\t3.6074\n",
            "source\t3.5977\n",
            "conservapedia\t3.5977\n",
            "followed\t3.4647\n",
            "https\t3.2275\n",
            "each\t3.1855\n",
            "page\t3.1855\n",
            "source_article\t3.0617\n",
            "\n",
            "Top {display} words aligned with class2:\n",
            "the\t-1.4772\n",
            "of\t-0.5481\n",
            "and\t-0.5208\n",
            "to\t-0.5049\n",
            "in\t-0.4911\n",
            "a\t-0.4852\n",
            "for\t-0.4275\n",
            "that\t-0.4191\n",
            "s\t-0.4079\n",
            "covid\t-0.4011\n",
            "climate\t-0.3964\n",
            "as\t-0.3940\n",
            "vaccine\t-0.3889\n",
            "world\t-0.3793\n",
            "are\t-0.3764\n",
            "on\t-0.3687\n",
            "global\t-0.3655\n",
            "it\t-0.3604\n",
            "american\t-0.3551\n",
            "change\t-0.3532\n",
            "school\t-0.3513\n",
            "not\t-0.3475\n",
            "coronavirus\t-0.3475\n",
            "has\t-0.3414\n",
            "warming\t-0.3414\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([('sa', 8.104814627744409),\n",
              "  ('0', 7.860570290019256),\n",
              "  ('cc', 6.254536619930356),\n",
              "  ('4', 5.843729849383912),\n",
              "  ('by', 5.624187625862183),\n",
              "  ('creativecommons', 5.099959726660873),\n",
              "  ('wikipedia', 5.099959726660873),\n",
              "  ('licenses', 5.099959726660873),\n",
              "  ('org', 4.796519676304396),\n",
              "  ('3', 4.169451530983527),\n",
              "  ('attribution', 3.607402953222064),\n",
              "  ('title', 3.607402953222064),\n",
              "  ('prefixed', 3.607402953222064),\n",
              "  ('section', 3.607402953222064),\n",
              "  ('dataset', 3.607402953222064),\n",
              "  ('licensing', 3.607402953222064),\n",
              "  ('txt', 3.607402953222064),\n",
              "  ('original', 3.607402953222064),\n",
              "  ('source', 3.5977293882165355),\n",
              "  ('conservapedia', 3.5977293882165355),\n",
              "  ('followed', 3.464745256886124),\n",
              "  ('https', 3.227465734731722),\n",
              "  ('each', 3.185526455047078),\n",
              "  ('page', 3.185526455047078),\n",
              "  ('source_article', 3.061650573881265)],\n",
              " [('the', -1.47716536013531),\n",
              "  ('of', -0.5481499917136652),\n",
              "  ('and', -0.5208181939758695),\n",
              "  ('to', -0.5049410084034835),\n",
              "  ('in', -0.49107774070468657),\n",
              "  ('a', -0.4851674220327707),\n",
              "  ('for', -0.4274706545853206),\n",
              "  ('that', -0.4191085396930662),\n",
              "  ('s', -0.40785308895598243),\n",
              "  ('covid', -0.40114231651397236),\n",
              "  ('climate', -0.39640780302587275),\n",
              "  ('as', -0.3939547745810198),\n",
              "  ('vaccine', -0.3888626308764373),\n",
              "  ('world', -0.3792830005528579),\n",
              "  ('are', -0.3763703543079625),\n",
              "  ('on', -0.36869831920249635),\n",
              "  ('global', -0.3654583361547844),\n",
              "  ('it', -0.36039440428553543),\n",
              "  ('american', -0.35506257499295213),\n",
              "  ('change', -0.3532207124877022),\n",
              "  ('school', -0.35134455084722355),\n",
              "  ('not', -0.34748402746307955),\n",
              "  ('coronavirus', -0.34748402746307955),\n",
              "  ('has', -0.34140072805538113),\n",
              "  ('warming', -0.34140072805538113)])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "logodds_with_uninformative_prior(class1_tokens, class2_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JfzZD9TaTwY"
      },
      "source": [
        "To check your work, you can run log-odds on the party platforms from the lab section. With `nltk.word_tokenize` _before_ lower-casing, these should be your top 5 words (and scores, roughly). Depending on your tokenization strategy, your scores might be slightly different.\n",
        "\n",
        "**Democrat**:\n",
        "```\n",
        "president:\t4.75\n",
        "biden:\t4.27\n",
        "to:\t4.11\n",
        "he:\t4.09\n",
        "has:\t4.08\n",
        "```\n",
        "**Republican**\n",
        "```\n",
        "republicans:\t-13.45\n",
        "our:\t-11.23\n",
        "will:\t-10.88\n",
        "american:\t-10.01\n",
        "restore:\t-7.97\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "leYX7OfgaTwY",
        "outputId": "ec187561-7dda-434e-f8ff-ad08d60b9189",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-09 17:11:03--  https://raw.githubusercontent.com/dbamman/anlp25/main/data/2024_democrat_party_platform.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 283046 (276K) [text/plain]\n",
            "Saving to: ‘2024_democrat_party_platform.txt’\n",
            "\n",
            "\r          2024_demo   0%[                    ]       0  --.-KB/s               \r2024_democrat_party 100%[===================>] 276.41K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-09-09 17:11:03 (7.23 MB/s) - ‘2024_democrat_party_platform.txt’ saved [283046/283046]\n",
            "\n",
            "--2025-09-09 17:11:03--  https://raw.githubusercontent.com/dbamman/anlp25/main/data/2024_republican_party_platform.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 35319 (34K) [text/plain]\n",
            "Saving to: ‘2024_republican_party_platform.txt’\n",
            "\n",
            "2024_republican_par 100%[===================>]  34.49K  --.-KB/s    in 0.009s  \n",
            "\n",
            "2025-09-09 17:11:03 (3.68 MB/s) - ‘2024_republican_party_platform.txt’ saved [35319/35319]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate https://raw.githubusercontent.com/dbamman/anlp25/main/data/2024_democrat_party_platform.txt\n",
        "!wget --no-check-certificate https://raw.githubusercontent.com/dbamman/anlp25/main/data/2024_republican_party_platform.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "65FvXwpnaTwZ",
        "outputId": "bedf20e3-2ce9-45d8-af5b-380b36d4f669",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 25 words aligned with class1:\n",
            "2024_democrat_party_platform.txt\t0.9185\n",
            "2024_republican_party_platform.txt\t-0.9185\n",
            "\n",
            "Top {display} words aligned with class2:\n",
            "2024_republican_party_platform.txt\t-0.9185\n",
            "2024_democrat_party_platform.txt\t0.9185\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([('2024_democrat_party_platform.txt', 0.9184883309391177),\n",
              "  ('2024_republican_party_platform.txt', -0.9184883309391177)],\n",
              " [('2024_republican_party_platform.txt', -0.9184883309391177),\n",
              "  ('2024_democrat_party_platform.txt', 0.9184883309391177)])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "logodds_with_uninformative_prior(\n",
        "    [w.lower() for w in nltk.word_tokenize(\"2024_democrat_party_platform.txt\")],\n",
        "    [w.lower() for w in nltk.word_tokenize(\"2024_republican_party_platform.txt\")]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZuIIkWuKaTwZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}